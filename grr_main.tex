\section{Kommunikationskostenoptimierung}
\hyphenation{High--Per-for-mance}
\hyphenation{Per-for-mance}
\hyphenation{Com-pu-ting}
\hyphenation{Be-rech-nungs-eng-pass}
Das Papier \cite{mainpaper} beschreibt in seinem dritten Kapitel Strategien zur Reduktion von Kommunikationskosten. Motiviert wird dies durch die Ergebnisse aus der Erfolgreichen GPU-Beschleunigung aus Kapitel 2.
Wie \cite[Abb. 3]{mainpaper} zeigt hat sich die Zeit, welche im FFT-Algorithmus f"ur Kommunikation zwischen den GPU's/CPU's aufgewendet wird, nur vernachl"assigbar ver"andert. Alle anderen Aktionen (Unpacking, Processing, Packing) haben sich erheblich durch Parallelisierung mit GPUs beschleunigt ($ \frac{0.74s}{0.017s} \approx 43.5$-fache Beschleunigung ).\\

Ein Berechnungsengpass durch Kommunikation von Applikationen in High-Performance-Computing(HPC)-Projekten, welche mehrere Prozessoren involvieren, scheint ein h"aufiges und bekanntes Problem zu sein.
%TODO quote references to similar bottlenecked projects
Dies kann dadurch begr"undet werden, dass Kommunikation "ublichweise genau dann unvermeidbar ist, wenn rein serielle Anteile in der Applikation/im Algorithmus present sind. Solche rein seriellen Anteile sind nicht parallelisierbar und ihre ergebnisse werden f"ur weitere Schritte ben"otigt, was Kommunikation in Multi-Prozessor-Systemen unvermeidbar macht.\\
%TODO cite sources
Um die Kommunikationskosten weiter zu optimieren steht eine Mischung aus folgenden Optionen zur Verf"ugung:
\begin{enumerate}
	\item Verwendung eines besseren Algorithmus hinsichtlich serieller Anteile und Kommunikation.
	\item Verbesserung der Kommunikationsstrategie unter Einbeziehung von Eigenschaften der Systemarchitektur.
\end{enumerate}

Die Schnelle Fouriertransformation (FFT) ist ein sehr spezifischer Algorithmus, was Verfolgungsm"oglichkeiten von Option 1 stark einschr"ankt. Dies mag der Grund daf"ur sein, weshalb sich \cite{mainpaper} auf Option 2 konzentriert.

\subsection{Relevante Technologien}
Das Papier benennt einige verwendete Technologien, gibt jedoch selbst dem Leser oft wenig Kontext zu diesen. Deshalb wird im Folgenden zu diesen Technologien der Kontext hergestellt.

\subsubsection{ NVIDIA GPUDirect }
	ist eine Technologie, die den direkten Zugriff von P"aripherie auf den Speicher von NVIDIA GPU's zul"asst. Dieser Vorgang wird als \textit{Remote Direct Memory Access} bezeichnet (RDMA).
		Dadurch wird der Umweg "uber traditionellen RAM vermieden. Beispiele für P"aripherien sind Netzwerkadapter, Speicher (wie Solid State Drives) und andere Graphikkarten. Es ist letztere Eigenschaft, welche im Kontext des Papiers vorrangig von Bedeutung ist (vgl. \cite{gpud}).

\subsubsection{ \textit{Message Passing Interface} (MPI) }
MPI ist ein Standard, der eine Schnittstelle für Nachrichten zwischen Prozessen spezifiziert. Portabilität und Einfachheit werden als prim"are Ziele angegeben (vgl. \cite[Kap. 1.1]{mpi}). Es gibt mehrere Implementierungen dieses Standards, zum Beispiel die Open-Source-Variante OpenMPI (\cite{openmpi}).

In verteilten Systemen wird MPI als Abstraktion für die Kommunikation zwischen parallelen Prozessoren/Prozessen verwendet.

\subsubsection{ CUDA }
Die \textit{\textbf{C}ompute \textbf{U}nified \textbf{D}evice \textbf{A}rchitecture}, entwickelt von NVIDIA ist eine Plattform und Programmiermodell f"ur parallele Computation auf NVIDIA GPUs. 

\subsubsection{ CUDA-aware MPI }
Ist eine Kombination aus CUDA und MPI. Da CUDA mit dem MPI Standard kompatibel ist, sind MPI-Implementierungen möglich, welche bessere Umsetzungen von MPI-Software auf NVIDIA GPUs erm"oglichen.

\subsection{Weitere Spezielle Terminologie}
\subsubsection{Sockel \textit{(eng. socket)}}
Ein Sockel fasst im Kontext von \cite{mainpaper} mehrere Prozessoren zusammen. Diese Zusammenfassung bringt, verursacht durch die Systemarchitektur, spezielle Eigenschaften bei Relationen der Prozessoren untereinander (hier: verfügbare Kommunikationstechnologie).\\
Folgende Terminologie wird dabei verwendet:
\begin{defi}[\textit{same-socket-communication}]
Ein Kommunikationsvorgang, welcher zwischen zwei Prozessoren unter dem selben Sockel erfolgt.
\end{defi}
\begin{defi}[\textit{cross-socket-communication}]
Ein Kommunikationsvorgang, welcher zwischen zwei Prozessoren erfolgt, welche sich nicht gemeinsam auf einem Sockel befinden.
\end{defi}

\subsubsection{Kommunikationsrichtung}
\begin{defi}[Unidirektionale Kommunikation (\textit{ eng. unidirectional communication })]
In einem abgeschlossenen Kommunikationsvorgang zwischen $p1$ und $p2$ werden Daten ausschlie"sslich von $p1$ nach $p2$ kommuniziert.
\end{defi}
\begin{defi}[Bidirektionale Kommunikation (\textit{ eng. bidirectional communication })]
In einem abgeschlossenen Kommunikationsvorgang zwischen $p1$ und $p2$ können Daten sowohl von $p1$ nach $p2$, als auch von $p2$ nach $p1$ ausgetauscht werden.
\end{defi}


\subsection{Systemarchitektur}
\cite[Abb. 1]{mainpaper} zeigt die generelle Systemarchitektur des Summit Supercomputers in Oak Ridge National Laboratory(ORNL), zu dem sich die vorgestellten Überlegungen im Papier zu relativieren scheinen. Die Grafik zeigt Knoten, welche "uber verschiedene Arten von Networking/Bussen verbunden sind. Diese verschiedenen Networking-Technologien gehen mit verschiedenen Übertragungseigenschaften (vorrangig Bandbreite einher). Ebenfalls zu erkennen is das Konzept eines Sockels (\textit{eng. socket}), welcher mehrere Prozessoren zusammenfasst. 

Innerhalb eines Sockels sind alle Prozessoren (7TF GPU's) Peer-to-Peer "uber die NVLINK Technologie verbunden (50GB/s Bandbreite). Same-Socket kommunikation k"onnte netzwerktopologisch also Gesamtinformation mit einer Bandbreite von $50Gb/s * \frac{p^2}{2}$ austauschen (Mit $p$ als die Anzahl der beteiligten Prozessoren auf dem Sockel).
%TODO check if with NVLINK 50Gb is the speed set for bidirectional communication
Zwischen den Sockeln exisitiert ein X-Bus (64GB/s).
Theoretisch k"onnen also maximal $\frac{64GB/s}{50GB/s} = 1.28$ Prozessoren cross-socket kommunizerien. Unter der Annahme, dass jeder Prozess im Schnitt mit jedem anderen Prozess gleich viel kommuniziert  entstehen demnach Kommunikationsengp"asse zuerst bei der cross-socket Kommunikation.\\
Bestimmte Kommunikationsstrategien verhalten sich also besser auf bestimmten Zielarchitekturen als andere.
\cite{mainpaper} versucht deshalb durch Benchmarking zu optimieren. 

\subsection{Benchmarking}
